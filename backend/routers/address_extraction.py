from fastapi import APIRouter, HTTPException
import httpx
import re
from bs4 import BeautifulSoup
from typing import Optional

router = APIRouter()

@router.get("/extract-address")
async def extract_address_from_yandex_link(url: str):
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç –∞–¥—Ä–µ—Å –∏–∑ —Å—Å—ã–ª–∫–∏ –Ω–∞ –Ø–Ω–¥–µ–∫—Å.–ö–∞—Ä—Ç—ã, –≤–∫–ª—é—á–∞—è –∫–æ—Ä–æ—Ç–∫–∏–µ —Å—Å—ã–ª–∫–∏
    """
    print(f"üîç –ü–æ–ª—É—á–µ–Ω –∑–∞–ø—Ä–æ—Å –Ω–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∞–¥—Ä–µ—Å–∞ –∏–∑ URL: {url}")
    
    try:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ —Å—Å—ã–ª–∫–∞ –Ω–∞ –Ø–Ω–¥–µ–∫—Å.–ö–∞—Ä—Ç—ã
        if not ('yandex.ru/maps' in url or 'maps.yandex.ru' in url):
            print("‚ùå –ù–µ–≤–µ—Ä–Ω–∞—è —Å—Å—ã–ª–∫–∞ –Ω–∞ –Ø–Ω–¥–µ–∫—Å.–ö–∞—Ä—Ç—ã")
            raise HTTPException(status_code=400, detail="–ù–µ–≤–µ—Ä–Ω–∞—è —Å—Å—ã–ª–∫–∞ –Ω–∞ –Ø–Ω–¥–µ–∫—Å.–ö–∞—Ä—Ç—ã")

        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∞–¥—Ä–µ—Å –∏–∑ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤ —Å—Å—ã–ª–æ–∫
        print("üîß –ù–∞—á–∏–Ω–∞–µ–º –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∞–¥—Ä–µ—Å–∞...")
        address = await extract_address_from_url(url)
        
        if address:
            print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –∏–∑–≤–ª–µ—á–µ–Ω –∞–¥—Ä–µ—Å: {address}")
            return {"success": True, "address": address}
        else:
            print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –∞–¥—Ä–µ—Å –∏–∑ —Å—Å—ã–ª–∫–∏")
            raise HTTPException(status_code=404, detail="–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –∞–¥—Ä–µ—Å –∏–∑ —Å—Å—ã–ª–∫–∏")
            
    except HTTPException:
        raise
    except Exception as e:
        print(f"üí• –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Å—Å—ã–ª–∫–∏: {str(e)}")
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Å—Å—ã–ª–∫–∏: {str(e)}")

async def extract_address_from_url(url: str) -> Optional[str]:
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç –∞–¥—Ä–µ—Å –∏–∑ URL –Ø–Ω–¥–µ–∫—Å.–ö–∞—Ä—Ç
    """
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —ç—Ç–æ –∫–æ—Ä–æ—Ç–∫–æ–π —Å—Å—ã–ª–∫–æ–π
    if '/maps/-/' in url:
        return await extract_address_from_short_link(url)
    else:
        return await extract_address_from_regular_link(url)

async def extract_address_from_regular_link(url: str) -> Optional[str]:
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç –∞–¥—Ä–µ—Å –∏–∑ –æ–±—ã—á–Ω–æ–π —Å—Å—ã–ª–∫–∏ –Ø–Ω–¥–µ–∫—Å.–ö–∞—Ä—Ç
    """
    try:
        # –ü–∞—Ä—Å–∏–º URL
        from urllib.parse import urlparse, parse_qs
        
        parsed = urlparse(url)
        query_params = parse_qs(parsed.query)
        
        # 1. –ü—ã—Ç–∞–µ–º—Å—è –∏–∑–≤–ª–µ—á—å –∏–∑ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞ text
        if 'text' in query_params:
            address = query_params['text'][0]
            print(f"–ò–∑–≤–ª–µ—á–µ–Ω –∞–¥—Ä–µ—Å –∏–∑ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞ text: {address}")
            return address
        
        print(f"–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–ø—Ä–æ—Å–∞: {query_params}")
        print(f"–ü—É—Ç—å URL: {parsed.path}")
        print(f"–ß–∞—Å—Ç–∏ –ø—É—Ç–∏: {path_parts}")
        
        # 2. –ü—ã—Ç–∞–µ–º—Å—è –∏–∑–≤–ª–µ—á—å –∏–∑ –ø—É—Ç–∏ URL
        path_parts = parsed.path.split('/')
        print(f"–†–∞–∑–æ–±—Ä–∞–Ω–Ω—ã–µ —á–∞—Å—Ç–∏ –ø—É—Ç–∏: {path_parts}")
        if len(path_parts) >= 4:
            possible_address = path_parts[3]
            if (possible_address and 
                possible_address not in ['moscow', 'spb', 'saint-petersburg'] and
                not possible_address.isdigit() and
                not possible_address.startswith('-/')):
                address = possible_address
                print(f"–ò–∑–≤–ª–µ—á–µ–Ω –∞–¥—Ä–µ—Å –∏–∑ –ø—É—Ç–∏ URL: {address}")
                return address
        
        # 3. –ï—Å–ª–∏ –µ—Å—Ç—å –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã, –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—Ä–∞—Ç–Ω–æ–µ –≥–µ–æ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ
        if 'll' in query_params:
            coords = query_params['ll'][0].split(',')
            if len(coords) == 2:
                print(f"–í—ã–ø–æ–ª–Ω—è–µ–º –æ–±—Ä–∞—Ç–Ω–æ–µ –≥–µ–æ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç: {coords[0]}, {coords[1]}")
                address = await reverse_geocode(coords[0], coords[1])
                if address:
                    print(f"–ü–æ–ª—É—á–µ–Ω –∞–¥—Ä–µ—Å —á–µ—Ä–µ–∑ –æ–±—Ä–∞—Ç–Ω–æ–µ –≥–µ–æ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ: {address}")
                    return address
        
        print("–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –∞–¥—Ä–µ—Å –∏–∑ —Å—Å—ã–ª–∫–∏")
        return None
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ –∞–¥—Ä–µ—Å–∞ –∏–∑ –æ–±—ã—á–Ω–æ–π —Å—Å—ã–ª–∫–∏: {e}")
        return None

async def extract_address_from_short_link(short_url: str) -> Optional[str]:
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç –∞–¥—Ä–µ—Å –∏–∑ –∫–æ—Ä–æ—Ç–∫–æ–π —Å—Å—ã–ª–∫–∏ —á–µ—Ä–µ–∑ –≤–µ–±-—Å–∫—Ä–∞–ø–∏–Ω–≥
    """
    print(f"üîç –ù–∞—á–∏–Ω–∞–µ–º –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∞–¥—Ä–µ—Å–∞ –∏–∑ –∫–æ—Ä–æ—Ç–∫–æ–π —Å—Å—ã–ª–∫–∏: {short_url}")
    
    try:
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –¥–ª—è –∏–º–∏—Ç–∞—Ü–∏–∏ –±—Ä–∞—É–∑–µ—Ä–∞
        headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ru-RU,ru;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        }
        
        async with httpx.AsyncClient(timeout=15.0, follow_redirects=True) as client:
            print("üì° –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å –∫ –Ø–Ω–¥–µ–∫—Å.–ö–∞—Ä—Ç–∞–º...")
            response = await client.get(short_url, headers=headers)
            response.raise_for_status()
            
            print(f"‚úÖ –ü–æ–ª—É—á–µ–Ω –æ—Ç–≤–µ—Ç: {response.status_code}")
            html = response.text
            print(f"üìÑ –†–∞–∑–º–µ—Ä HTML: {len(html)} —Å–∏–º–≤–æ–ª–æ–≤")
            
            # –ü–∞—Ä—Å–∏–º HTML
            soup = BeautifulSoup(html, 'html.parser')
            
            # –ò—â–µ–º –∞–¥—Ä–µ—Å –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–µ—Å—Ç–∞—Ö
            address = None
            
            # 1. –ò—â–µ–º –≤ –º–µ—Ç–∞-—Ç–µ–≥–∞—Ö
            print("üîç –ü–æ–∏—Å–∫ –≤ –º–µ—Ç–∞-—Ç–µ–≥–∞—Ö...")
            meta_desc = soup.find('meta', property='og:description')
            if meta_desc and meta_desc.get('content'):
                content = meta_desc['content']
                print(f"üìù –ù–∞–π–¥–µ–Ω og:description: {content}")
                match = re.search(r'–∞–¥—Ä–µ—Å[:\s]+([^,]+)', content, re.IGNORECASE)
                if match:
                    address = match.group(1).strip()
                    print(f"‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω –∞–¥—Ä–µ—Å –∏–∑ og:description: {address}")
            
            # 2. –ò—â–µ–º –≤ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            if not address:
                print("üîç –ü–æ–∏—Å–∫ –≤ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
                scripts = soup.find_all('script', type='application/ld+json')
                for script in scripts:
                    try:
                        import json
                        data = json.loads(script.string)
                        if 'address' in data and 'streetAddress' in data['address']:
                            address = data['address']['streetAddress']
                            print(f"‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω –∞–¥—Ä–µ—Å –∏–∑ JSON-LD: {address}")
                            break
                    except:
                        continue
            
            # 3. –ò—â–µ–º –≤ —Ç–µ–∫—Å—Ç–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
            if not address:
                print("üîç –ü–æ–∏—Å–∫ –≤ —Ç–µ–∫—Å—Ç–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã...")
                text_content = soup.get_text()
                patterns = [
                    r'–∞–¥—Ä–µ—Å[:\s]+([^,\n]+)',
                    r'–Ω–∞—Ö–æ–¥–∏—Ç—Å—è[:\s]+([^,\n]+)',
                    r'—Ä–∞—Å–ø–æ–ª–æ–∂–µ–Ω[:\s]+([^,\n]+)',
                    r'([^,\n]+—É–ª–∏—Ü–∞[^,\n]+)',
                    r'([^,\n]+–ø—Ä–æ—Å–ø–µ–∫—Ç[^,\n]+)',
                    r'([^,\n]+–ø–µ—Ä–µ—É–ª–æ–∫[^,\n]+)',
                ]
                
                for pattern in patterns:
                    match = re.search(pattern, text_content, re.IGNORECASE)
                    if match:
                        address = match.group(1).strip()
                        print(f"‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω –∞–¥—Ä–µ—Å –∏–∑ —Ç–µ–∫—Å—Ç–∞: {address}")
                        break
            
            # 4. –ò—â–µ–º –≤ –∑–∞–≥–æ–ª–æ–≤–∫–∞—Ö
            if not address:
                print("üîç –ü–æ–∏—Å–∫ –≤ –∑–∞–≥–æ–ª–æ–≤–∫–∞—Ö...")
                headings = soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])
                for heading in headings:
                    text = heading.get_text()
                    if '–∞–¥—Ä–µ—Å' in text.lower():
                        next_element = heading.find_next_sibling()
                        if next_element and next_element.get_text():
                            address = next_element.get_text().strip()
                            print(f"‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω –∞–¥—Ä–µ—Å –∏–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞: {address}")
                            break
            
            # 5. –ò—â–µ–º –≤ –∫–∞—Ä—Ç–æ—á–∫–µ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏
            if not address:
                print("üîç –ü–æ–∏—Å–∫ –≤ –∫–∞—Ä—Ç–æ—á–∫–µ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏...")
                # –ò—â–µ–º —ç–ª–µ–º–µ–Ω—Ç—ã —Å –∫–ª–∞—Å—Å–∞–º–∏, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç —Å–æ–¥–µ—Ä–∂–∞—Ç—å –∞–¥—Ä–µ—Å
                address_elements = soup.find_all(['div', 'span', 'p'], 
                                               class_=re.compile(r'address|–∞–¥—Ä–µ—Å|location', re.IGNORECASE))
                for element in address_elements:
                    text = element.get_text().strip()
                    if text and len(text) > 10:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –¥–ª—è –∞–¥—Ä–µ—Å–∞
                        address = text
                        print(f"‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω –∞–¥—Ä–µ—Å –∏–∑ –∫–∞—Ä—Ç–æ—á–∫–∏: {address}")
                        break
            
            if address:
                print(f"üéâ –£–°–ü–ï–•: –ù–∞–π–¥–µ–Ω –∞–¥—Ä–µ—Å: {address}")
            else:
                print("‚ùå –ê–¥—Ä–µ—Å –Ω–µ –Ω–∞–π–¥–µ–Ω")
            
            return address
            
    except Exception as e:
        print(f"üí• –û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ –∞–¥—Ä–µ—Å–∞ –∏–∑ –∫–æ—Ä–æ—Ç–∫–æ–π —Å—Å—ã–ª–∫–∏: {e}")
        return None

async def reverse_geocode(lon: str, lat: str) -> Optional[str]:
    """
    –í—ã–ø–æ–ª–Ω—è–µ—Ç –æ–±—Ä–∞—Ç–Ω–æ–µ –≥–µ–æ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–∞–º
    """
    try:
        import httpx
        
        url = f"https://geocode-maps.yandex.ru/1.x/"
        params = {
            "format": "json",
            "geocode": f"{lon},{lat}",
            "lang": "ru_RU"
        }
        
        async with httpx.AsyncClient(timeout=10.0) as client:
            response = await client.get(url, params=params)
            response.raise_for_status()
            
            data = response.json()
            feature_member = data['response']['GeoObjectCollection']['featureMember']
            
            if feature_member:
                return feature_member[0]['GeoObject']['metaDataProperty']['GeocoderMetaData']['text']
        
        return None
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞—Ç–Ω–æ–º –≥–µ–æ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–∏: {e}")
        return None 